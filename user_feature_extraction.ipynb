{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c2fcc4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import prawcore\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import chain, combinations\n",
    "from more_itertools import pairwise\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, date\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "from spacy import load\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=r\"\\[W008\\]\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802e9bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ray\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_toxicity = pipeline(\"sentiment-analysis\", model=\"unitary/toxic-bert\")\n",
    "\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "for key in data.keys():\n",
    "    client_id = data['client_id']\n",
    "    client_secret = data['client_secret']\n",
    "    user_agent = data['user_agent']\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6dcd79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "user_dir = 'user_data'\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f96ec0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    norm_text = []\n",
    "\n",
    "    for token in text:\n",
    "        if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "            norm_text.append(token.lemma_.lower())\n",
    "\n",
    "    norm_text = ' '.join(norm_text)\n",
    "    norm_text = re.sub(r'(?:^| )\\w(?:$| )', ' ', norm_text).strip()  # removes single characters\n",
    "    norm_text = re.sub(r'[^a-zA-Z0-9 ]', '', norm_text)\n",
    "    \n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac2dd46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_user_status(path):\n",
    "    try:\n",
    "        user.is_suspended\n",
    "        user_status = 'suspended'\n",
    "    except AttributeError:\n",
    "        user_status = 'active'\n",
    "    except:\n",
    "        user_status = 'deleted'\n",
    "    return user_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8965780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(a,b):\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    if b == 0:\n",
    "        return a\n",
    "    else:\n",
    "        return ratio(b, a % b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "415418ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_sim(sent_1, sent_2):\n",
    "    l1 = []; l2 = []\n",
    "    \n",
    "    sent_1 = set(sent_1.split(' '))\n",
    "    sent_2 = set(sent_2.split(' '))\n",
    "    \n",
    "    rvector = sent_1.union(sent_2)\n",
    "    \n",
    "    for word in rvector:\n",
    "        if word in sent_1: l1.append(1)\n",
    "        else: l1.append(0)\n",
    "        if word in sent_2: l2.append(1)\n",
    "        else: l2.append(0)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(rvector)):\n",
    "        count += l1[i] * l2[i]\n",
    "    cosine = count/float((sum(l1) * sum(l2))**.5)\n",
    "    \n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30d0c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    WORD = re.compile(r\"\\w+\")\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d328ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentences:\n",
      "the fox jumped over the log\n",
      "the dog sat on the log\n",
      "\n",
      "normalized sentences:\n",
      "fox jump log\n",
      "dog sit log\n",
      "\n",
      "cosine_function: 0.3333333333333333\n",
      "spacy similarity function: 0.7902248506898802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ray\\AppData\\Local\\Temp/ipykernel_30268/2116112219.py:24: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(f'spacy similarity function: {sent_1.similarity(sent_2)}')\n"
     ]
    }
   ],
   "source": [
    "sent_1 = \"the fox jumped over the log\"\n",
    "sent_2 = \"the dog sat on the log\"\n",
    "\n",
    "print('original sentences:')\n",
    "print(sent_1)\n",
    "print(sent_2)\n",
    "print()\n",
    "\n",
    "sent_1 = normalize(nlp(sent_1))\n",
    "sent_2 = normalize(nlp(sent_2))\n",
    "\n",
    "vec_1 = text_to_vector(sent_1)\n",
    "vec_2 = text_to_vector(sent_2)\n",
    "print('normalized sentences:')\n",
    "print (sent_1)\n",
    "print (sent_2)\n",
    "print()\n",
    "\n",
    "print(f'cosine_function: {calculate_cosine_sim(sent_1, sent_2)}')\n",
    "# print(f'cosine_function_2: {get_cosine(vec_1, vec_2)}')\n",
    "\n",
    "sent_1 = nlp(sent_1)\n",
    "sent_2 = nlp(sent_2)\n",
    "print(f'spacy similarity function: {sent_1.similarity(sent_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c12675",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7db364f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'author'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30268/2934287551.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mfile_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0muser_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0musers_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'comment_simularity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5905\u001b[0m         ):\n\u001b[0;32m   5906\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5907\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5909\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'author'"
     ]
    }
   ],
   "source": [
    "for filename in os.scandir('data'):\n",
    "    if filename.is_file():\n",
    "        \n",
    "        file_df = pd.read_csv(filename)\n",
    "        user_names = file_df.author\n",
    "        users_df = pd.DataFrame(columns=['user_name', 'comment_simularity'])\n",
    "        rows = []\n",
    "        \n",
    "        for user_name in tqdm(user_names[:1000]):\n",
    "            try:\n",
    "                user_dict = {}\n",
    "                user_dict['username'] = user_name\n",
    "                user = reddit.redditor(user_name)\n",
    "\n",
    "                if get_user_status(user) == 'active' and str(user_name) not in 'nan': # checks that user isn't suspended/deleted\n",
    "                    if not user.is_mod: #ignore mods\n",
    "\n",
    "                        comment_array = []\n",
    "                        timestamps = []\n",
    "                        reply_timestamps = []\n",
    "\n",
    "                        try:\n",
    "                            for this_comment in user.comments.new(limit=10):\n",
    "\n",
    "                                parent_comment_id = this_comment.parent_id\n",
    "                                if parent_comment_id.startswith('t3'):\n",
    "                                    parent_comment_id = parent_comment_id[3:]\n",
    "                                    parent = reddit.submission(parent_comment_id)\n",
    "                                else:\n",
    "                                    parent = reddit.comment(parent_comment_id)\n",
    "\n",
    "                                parent_timestamp = datetime.fromtimestamp(parent.created_utc)\n",
    "                                comment_timestamp = datetime.fromtimestamp(this_comment.created_utc)\n",
    "\n",
    "                                comment_array.append(this_comment.body)\n",
    "                                timestamps.append(comment_timestamp)\n",
    "                                reply_timestamps.append((parent_timestamp, comment_timestamp))\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "\n",
    "                        for i in range(len(comment_array)):\n",
    "                            comment_array[i] = normalize(nlp((comment_array[i]))) # normalizes comment but leaves as string\n",
    "                            comment_array[i] = nlp(normalize(nlp((comment_array[i])))) # this normalizes comment and wraps in nlp\n",
    "\n",
    "                        comment_similarities = []\n",
    "                        time_intervals = []\n",
    "                        response_intervals = []\n",
    "\n",
    "                        for sent_1, sent_2 in combinations(comment_array, 2):\n",
    "\n",
    "    #                         comment_similarities.append(calculate_cosine_sim(sent_1, sent_2)) checks cosine simularity of each comment against the next \n",
    "                            comment_similarities.append(sent_1.similarity(sent_2)) # nlp similiarity\n",
    "\n",
    "                        successive_times = list(pairwise(timestamps))\n",
    "                        for pair in successive_times: # calculates the intervals between user's comments\n",
    "                            time_intervals.append(abs(pair[0] - pair[1]))\n",
    "\n",
    "                        for pair in reply_timestamps:\n",
    "                            response_intervals.append(abs(pair[1] - pair[0]))# calculates how quickly a comment replied to its parent\n",
    "\n",
    "                        try:\n",
    "                            time_data = pd.Series(time_intervals)\n",
    "                            avg_time_diff = (time_data.sum()/len(time_data)).round('1s')\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            response_data = pd.Series(response_intervals)\n",
    "                            avg_reply_speed = (response_data.sum()/len(response_data)).round('1s')\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            avg_comment_similarity = sum(comment_similarities)/len(comment_similarities)\n",
    "                        except ZeroDivisionError:\n",
    "                            continue\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "                        try:\n",
    "                            user_dict['avg_comment_similarity'] = avg_comment_similarity # formating dict\n",
    "                            user_dict['avg_comment_time_interval'] = avg_time_diff\n",
    "                            user_dict['avg_reply_speed'] = avg_reply_speed\n",
    "                            user_dict['avg_reply_speed'] = avg_reply_speed\n",
    "                            user_dict['reply_speeds'] = response_intervals\n",
    "\n",
    "                            rows.append(user_dict) \n",
    "                        except Exeception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "                    \n",
    "        users_df = pd.DataFrame.from_dict(rows, orient='columns')\n",
    "        print(users_df)\n",
    "        \n",
    "        filename = str(filename)\n",
    "        \n",
    "        if filename.startswith(\"<DirEntry 'comment\"):\n",
    "            users_df.to_csv(os.path.join(f'{path}/{user_dir}', f'raw_users_{filename[19:-2]}'), index=False)\n",
    "\n",
    "        if filename.startswith(\"<DirEntry 'submission\"):\n",
    "            users_df.to_csv(os.path.join(f'{path}/{user_dir}', f'raw_users_{filename[22:-2]}'), header=False, index=False, mode='a')\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0d3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
