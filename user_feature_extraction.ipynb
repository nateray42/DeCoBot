{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c2fcc4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import prawcore\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import chain, combinations\n",
    "from more_itertools import pairwise\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, date\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "from spacy import load\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=r\"\\[W008\\]\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802e9bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ray\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_toxicity = pipeline(\"sentiment-analysis\", model=\"unitary/toxic-bert\")\n",
    "\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6dcd79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "user_dir = 'user_data'\n",
    "reddit = praw.Reddit(\n",
    "    client_id='4bDjCY6y8ncrc3kLrBbpBg',\n",
    "    client_secret='zPHZKfk9S666S9IxR5HtvkZ83ufNxw',\n",
    "    user_agent='webcrawler created for IS596'\n",
    ")\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f96ec0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    norm_text = []\n",
    "\n",
    "    for token in text:\n",
    "        if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "            norm_text.append(token.lemma_.lower())\n",
    "\n",
    "    norm_text = ' '.join(norm_text)\n",
    "    norm_text = re.sub(r'(?:^| )\\w(?:$| )', ' ', norm_text).strip()  # removes single characters\n",
    "    norm_text = re.sub(r'[^a-zA-Z0-9 ]', '', norm_text)\n",
    "    \n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac2dd46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_user_status(path):\n",
    "    try:\n",
    "        user.is_suspended\n",
    "        user_status = 'suspended'\n",
    "    except AttributeError:\n",
    "        user_status = 'active'\n",
    "    except:\n",
    "        user_status = 'deleted'\n",
    "    return user_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8965780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(a,b):\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    if b == 0:\n",
    "        return a\n",
    "    else:\n",
    "        return ratio(b, a % b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "415418ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_sim(sent_1, sent_2):\n",
    "    l1 = []; l2 = []\n",
    "    \n",
    "    sent_1 = set(sent_1.split(' '))\n",
    "    sent_2 = set(sent_2.split(' '))\n",
    "    \n",
    "    rvector = sent_1.union(sent_2)\n",
    "    \n",
    "    for word in rvector:\n",
    "        if word in sent_1: l1.append(1)\n",
    "        else: l1.append(0)\n",
    "        if word in sent_2: l2.append(1)\n",
    "        else: l2.append(0)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(rvector)):\n",
    "        count += l1[i] * l2[i]\n",
    "    cosine = count/float((sum(l1) * sum(l2))**.5)\n",
    "    \n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30d0c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    WORD = re.compile(r\"\\w+\")\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d328ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentences:\n",
      "the fox jumped over the log\n",
      "the dog sat on the log\n",
      "\n",
      "normalized sentences:\n",
      "fox jump log\n",
      "dog sit log\n",
      "\n",
      "cosine_function: 0.3333333333333333\n",
      "spacy similarity function: 0.7902248506898802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ray\\AppData\\Local\\Temp/ipykernel_30268/2116112219.py:24: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(f'spacy similarity function: {sent_1.similarity(sent_2)}')\n"
     ]
    }
   ],
   "source": [
    "sent_1 = \"the fox jumped over the log\"\n",
    "sent_2 = \"the dog sat on the log\"\n",
    "\n",
    "print('original sentences:')\n",
    "print(sent_1)\n",
    "print(sent_2)\n",
    "print()\n",
    "\n",
    "sent_1 = normalize(nlp(sent_1))\n",
    "sent_2 = normalize(nlp(sent_2))\n",
    "\n",
    "vec_1 = text_to_vector(sent_1)\n",
    "vec_2 = text_to_vector(sent_2)\n",
    "print('normalized sentences:')\n",
    "print (sent_1)\n",
    "print (sent_2)\n",
    "print()\n",
    "\n",
    "print(f'cosine_function: {calculate_cosine_sim(sent_1, sent_2)}')\n",
    "# print(f'cosine_function_2: {get_cosine(vec_1, vec_2)}')\n",
    "\n",
    "sent_1 = nlp(sent_1)\n",
    "sent_2 = nlp(sent_2)\n",
    "print(f'spacy similarity function: {sent_1.similarity(sent_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c12675",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7db364f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'author'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30268/2934287551.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mfile_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0muser_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0musers_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'comment_simularity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5905\u001b[0m         ):\n\u001b[0;32m   5906\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5907\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5909\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'author'"
     ]
    }
   ],
   "source": [
    "for filename in os.scandir('data'):\n",
    "    if filename.is_file():\n",
    "        \n",
    "        file_df = pd.read_csv(filename)\n",
    "        user_names = file_df.author\n",
    "        users_df = pd.DataFrame(columns=['user_name', 'comment_simularity'])\n",
    "        rows = []\n",
    "        \n",
    "        for user_name in tqdm(user_names[:1000]):\n",
    "            try:\n",
    "                user_dict = {}\n",
    "                user_dict['username'] = user_name\n",
    "                user = reddit.redditor(user_name)\n",
    "\n",
    "                if get_user_status(user) == 'active' and str(user_name) not in 'nan': # checks that user isn't suspended/deleted\n",
    "                    if not user.is_mod: #ignore mods\n",
    "\n",
    "                        comment_array = []\n",
    "                        timestamps = []\n",
    "                        reply_timestamps = []\n",
    "\n",
    "                        try:\n",
    "                            for this_comment in user.comments.new(limit=10):\n",
    "\n",
    "                                parent_comment_id = this_comment.parent_id\n",
    "                                if parent_comment_id.startswith('t3'):\n",
    "                                    parent_comment_id = parent_comment_id[3:]\n",
    "                                    parent = reddit.submission(parent_comment_id)\n",
    "                                else:\n",
    "                                    parent = reddit.comment(parent_comment_id)\n",
    "\n",
    "                                parent_timestamp = datetime.fromtimestamp(parent.created_utc)\n",
    "                                comment_timestamp = datetime.fromtimestamp(this_comment.created_utc)\n",
    "\n",
    "                                comment_array.append(this_comment.body)\n",
    "                                timestamps.append(comment_timestamp)\n",
    "                                reply_timestamps.append((parent_timestamp, comment_timestamp))\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "\n",
    "                        for i in range(len(comment_array)):\n",
    "                            comment_array[i] = normalize(nlp((comment_array[i]))) # normalizes comment but leaves as string\n",
    "                            comment_array[i] = nlp(normalize(nlp((comment_array[i])))) # this normalizes comment and wraps in nlp\n",
    "\n",
    "                        comment_similarities = []\n",
    "                        time_intervals = []\n",
    "                        response_intervals = []\n",
    "\n",
    "                        for sent_1, sent_2 in combinations(comment_array, 2):\n",
    "\n",
    "    #                         comment_similarities.append(calculate_cosine_sim(sent_1, sent_2)) checks cosine simularity of each comment against the next \n",
    "                            comment_similarities.append(sent_1.similarity(sent_2)) # nlp similiarity\n",
    "\n",
    "                        successive_times = list(pairwise(timestamps))\n",
    "                        for pair in successive_times: # calculates the intervals between user's comments\n",
    "                            time_intervals.append(abs(pair[0] - pair[1]))\n",
    "\n",
    "                        for pair in reply_timestamps:\n",
    "                            response_intervals.append(abs(pair[1] - pair[0]))# calculates how quickly a comment replied to its parent\n",
    "\n",
    "                        try:\n",
    "                            time_data = pd.Series(time_intervals)\n",
    "                            avg_time_diff = (time_data.sum()/len(time_data)).round('1s')\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            response_data = pd.Series(response_intervals)\n",
    "                            avg_reply_speed = (response_data.sum()/len(response_data)).round('1s')\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            avg_comment_similarity = sum(comment_similarities)/len(comment_similarities)\n",
    "                        except ZeroDivisionError:\n",
    "                            continue\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "                        try:\n",
    "                            user_dict['avg_comment_similarity'] = avg_comment_similarity # formating dict\n",
    "                            user_dict['avg_comment_time_interval'] = avg_time_diff\n",
    "                            user_dict['avg_reply_speed'] = avg_reply_speed\n",
    "                            user_dict['avg_reply_speed'] = avg_reply_speed\n",
    "                            user_dict['reply_speeds'] = response_intervals\n",
    "\n",
    "                            rows.append(user_dict) \n",
    "                        except Exeception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "                    \n",
    "        users_df = pd.DataFrame.from_dict(rows, orient='columns')\n",
    "        print(users_df)\n",
    "        \n",
    "        filename = str(filename)\n",
    "        \n",
    "        if filename.startswith(\"<DirEntry 'comment\"):\n",
    "            users_df.to_csv(os.path.join(f'{path}/{user_dir}', f'raw_users_{filename[19:-2]}'), index=False)\n",
    "\n",
    "        if filename.startswith(\"<DirEntry 'submission\"):\n",
    "            users_df.to_csv(os.path.join(f'{path}/{user_dir}', f'raw_users_{filename[22:-2]}'), header=False, index=False, mode='a')\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a6e2d",
   "metadata": {},
   "source": [
    "## Troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f8a981",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment id: izkdtip\n",
      "comment timestamp: 2022-12-09 13:15:00\n",
      " It's so weird that they removed it. Here forehead is just empty now.\n",
      "parent_comment_id: t1_izjn7a3\n",
      "parent timestamp: 2022-12-09 10:27:04\n",
      "\n",
      "comment id: izjvq2s\n",
      "comment timestamp: 2022-12-09 11:21:38\n",
      "Haha! Is it a foster situation or are they all hers?\n",
      "parent_comment_id: t1_izie9y8\n",
      "parent timestamp: 2022-12-09 03:33:18\n",
      "\n",
      "comment id: izjvib3\n",
      "comment timestamp: 2022-12-09 11:20:16\n",
      " They eventually shed the velvet.\n",
      "parent_comment_id: t1_izj2w98\n",
      "parent timestamp: 2022-12-09 08:05:36\n",
      "\n",
      "comment id: izjvb46\n",
      "comment timestamp: 2022-12-09 11:19:02\n",
      " We love you guys!\n",
      "parent_comment_id: t1_izjcgsc\n",
      "parent timestamp: 2022-12-09 09:15:07\n",
      "\n",
      "comment id: izjv0bg\n",
      "comment timestamp: 2022-12-09 11:17:08\n",
      ". Although, surprisingly, it stuck last night. Caused havoc on the traffic this morning; we're rain folk.\n",
      "parent_comment_id: t1_izj9l8f\n",
      "parent timestamp: 2022-12-09 08:54:50\n",
      "\n",
      "reply times:\n",
      "(datetime.datetime(2022, 12, 9, 10, 27, 4), datetime.datetime(2022, 12, 9, 13, 15))\n",
      "(datetime.datetime(2022, 12, 9, 3, 33, 18), datetime.datetime(2022, 12, 9, 11, 21, 38))\n",
      "(datetime.datetime(2022, 12, 9, 8, 5, 36), datetime.datetime(2022, 12, 9, 11, 20, 16))\n",
      "(datetime.datetime(2022, 12, 9, 9, 15, 7), datetime.datetime(2022, 12, 9, 11, 19, 2))\n",
      "(datetime.datetime(2022, 12, 9, 8, 54, 50), datetime.datetime(2022, 12, 9, 11, 17, 8))\n",
      "\n",
      "response intervals:\n",
      "2:47:56\n",
      "7:48:20\n",
      "3:14:40\n",
      "2:03:55\n",
      "2:22:18\n"
     ]
    }
   ],
   "source": [
    "user = reddit.redditor('Excellenepperg')\n",
    "comment_array = []\n",
    "timestamps = []\n",
    "reply_times = []\n",
    "\n",
    "for this_comment in user.comments.new(limit=5):\n",
    "                            \n",
    "    comment_timestamp = datetime.fromtimestamp(this_comment.created_utc)\n",
    "    print(f'comment id: {this_comment.id}')\n",
    "    print(f'comment timestamp: {comment_timestamp}')\n",
    "    parent_comment_id = this_comment.parent_id\n",
    "    \n",
    "    print(this_comment.body)\n",
    "    if parent_comment_id.startswith('t3'):\n",
    "        parent_comment_id = parent_comment_id[3:]\n",
    "        parent = reddit.submission(parent_comment_id)\n",
    "        print(f'parent_comment_id: {parent.id}')\n",
    "    else:\n",
    "        parent = reddit.comment(parent_comment_id)\n",
    "        print(f'parent_comment_id: {parent.id}')\n",
    "    try:\n",
    "        parent_id = parent.id.lstrip('t3_')\n",
    "        parent_timestamp = datetime.fromtimestamp(parent.created_utc)\n",
    "        print(f'parent timestamp: {parent_timestamp}')\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print()\n",
    "        continue\n",
    "    comment_array.append(this_comment.body)\n",
    "    timestamps.append(comment_timestamp)\n",
    "    reply_times.append((parent_timestamp, comment_timestamp))\n",
    "\n",
    "print(f'reply times:')\n",
    "for reply in reply_times:\n",
    "    print(reply)\n",
    "response_intervals = []\n",
    "for pair in reply_times:\n",
    "    response_intervals.append(abs(pair[1] - pair[0]))\n",
    "\n",
    "print()\n",
    "print('response intervals:')\n",
    "for response in response_intervals:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db9746c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "praw.models.listing.mixins.base.BaseListingMixin.new() got multiple values for keyword argument 'limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m botdefense \u001b[38;5;241m=\u001b[39m reddit\u001b[38;5;241m.\u001b[39msubreddit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbotdefense\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m submission \u001b[38;5;129;01min\u001b[39;00m botdefense\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39msubmissions(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m         user_name \u001b[38;5;241m=\u001b[39m submission\u001b[38;5;241m.\u001b[39mtitle[\u001b[38;5;241m13\u001b[39m:]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\IS_596\\lib\\site-packages\\praw\\models\\util.py:195\u001b[0m, in \u001b[0;36mstream_generator\u001b[1;34m(function, attribute_name, exclude_before, pause_after, skip_existing, **function_kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exclude_before:\n\u001b[0;32m    194\u001b[0m     function_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m\"\u001b[39m: before_attribute}\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(function(limit\u001b[38;5;241m=\u001b[39mlimit, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunction_kwargs))):\n\u001b[0;32m    196\u001b[0m     attribute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(item, attribute_name)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attribute \u001b[38;5;129;01min\u001b[39;00m seen_attributes:\n",
      "\u001b[1;31mTypeError\u001b[0m: praw.models.listing.mixins.base.BaseListingMixin.new() got multiple values for keyword argument 'limit'"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for submission in botdefense.stream.submissions():\n",
    "    try:\n",
    "        user_name = submission.title[13:]\n",
    "\n",
    "        user_dict = {}\n",
    "        user_dict['username'] = user_name\n",
    "        user = reddit.redditor(user_name)\n",
    "\n",
    "        comment_karma = user.comment_karma\n",
    "        submission_karma = user.link_karma\n",
    "\n",
    "        karma_ratio = round(comment_karma/submission_karma, 4)\n",
    "\n",
    "        comment_array = []\n",
    "        timestamps = []\n",
    "        reply_timestamps = []\n",
    "        comment_toxicities = []\n",
    "\n",
    "\n",
    "        for this_comment in user.comments.new(limit=15):\n",
    "\n",
    "            parent_comment_id = this_comment.parent_id\n",
    "            if parent_comment_id.startswith('t3'):\n",
    "                parent_comment_id = parent_comment_id[3:]\n",
    "                parent = reddit.submission(parent_comment_id)\n",
    "            else:\n",
    "                parent = reddit.comment(parent_comment_id)\n",
    "\n",
    "            parent_timestamp = datetime.fromtimestamp(parent.created_utc)\n",
    "            comment_timestamp = datetime.fromtimestamp(this_comment.created_utc)\n",
    "\n",
    "            comment_array.append(this_comment.body)\n",
    "            comment_toxicities.append(sentiment_toxicity(this_comment.body, **tokenizer_kwargs)[0]['score'])\n",
    "\n",
    "            timestamps.append(comment_timestamp)\n",
    "            reply_timestamps.append((parent_timestamp, comment_timestamp))\n",
    "\n",
    "        for i in range(len(comment_array)):\n",
    "            comment_array[i] = nlp(normalize(nlp((comment_array[i]))))\n",
    "\n",
    "        comment_similarities = []\n",
    "        time_intervals = []\n",
    "        response_intervals = []\n",
    "\n",
    "        for sent_1, sent_2 in combinations(comment_array, 2):\n",
    "            comment_similarities.append(sent_1.similarity(sent_2))\n",
    "\n",
    "        successive_times = list(pairwise(timestamps))\n",
    "        for pair in successive_times: # calculates the intervals between user's comments\n",
    "            time_intervals.append(abs(pair[0] - pair[1]))\n",
    "        for pair in reply_timestamps:\n",
    "            response_intervals.append(abs(pair[1] - pair[0]))\n",
    "\n",
    "        time_data = pd.Series(time_intervals)\n",
    "        avg_time_diff = (time_data.sum()/len(time_data)).round('1s')\n",
    "\n",
    "        response_data = pd.Series(response_intervals)\n",
    "        avg_reply_speed = (response_data.sum()/len(response_data)).round('1s')\n",
    "\n",
    "        avg_comment_similarity = sum(comment_similarities)/len(comment_similarities)\n",
    "        avg_toxicity = sum(comment_toxicities)/len(comment_toxicities)\n",
    "\n",
    "        user_dict['comment_karma'] = comment_karma\n",
    "        user_dict['submission_karma'] = submission_karma\n",
    "        user_dict['karma_ratio'] = karma_ratio\n",
    "        user_dict['avg_comment_similarity'] = avg_comment_similarity # formating dict\n",
    "        user_dict['avg_toxicity'] = avg_toxicity\n",
    "        user_dict['avg_reply_speed'] = avg_reply_speed\n",
    "        user_dict['avg_comment_time_interval'] = avg_time_diff\n",
    "\n",
    "        print(user_dict)\n",
    "        rows.append(user_dict) \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "users_df = pd.DataFrame.from_dict(rows, orient='columns')\n",
    "users_df.to_csv(f'{path}/{user_dir}/suspected_bots_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0d3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
